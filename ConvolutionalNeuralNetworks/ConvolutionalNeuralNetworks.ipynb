{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 7. Build your own CNN and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist # load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # split dataset into training and testing sets\n",
    "# normalize the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "# reshape the data for the convolutional neural network\n",
    "x_train= x_train.reshape((-1,28,28,1)).astype('float32')\n",
    "x_test= x_test.reshape((-1,28,28,1)).astype('float32')\n",
    "# one hot encode the labels\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters=(32, 64, 64), dense_units=64, learning_rate=0.001): # function to create a model\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(filters[0], (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(filters[1], (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(filters[2], (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense_units, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameters\n",
    "param_grid = {\n",
    "    'filters': [(32, 64, 64), (64, 128, 128)],\n",
    "    'dense_units': [64, 128],\n",
    "    'learning_rate': [0.001, 0.0005],\n",
    "    'batch_size': [32,64, 128]\n",
    "}\n",
    "grid = ParameterGrid(param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(params):\n",
    "    print(f\"Training with params: {params}\")\n",
    "    model = create_model(filters=params['filters'], \n",
    "                         dense_units=params['dense_units'], \n",
    "                         learning_rate=params['learning_rate'])\n",
    "    history = model.fit(\n",
    "        datagen.flow(x_train, y_train, batch_size=params['batch_size']),\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=5,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=2\n",
    "    )\n",
    "    _, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return test_acc, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'batch_size': 32, 'dense_units': 64, 'filters': (32, 64, 64), 'learning_rate': 0.001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 29s - 15ms/step - accuracy: 0.9153 - loss: 0.2704 - val_accuracy: 0.9833 - val_loss: 0.0510\n",
      "Epoch 2/5\n",
      "1875/1875 - 24s - 13ms/step - accuracy: 0.9725 - loss: 0.0893 - val_accuracy: 0.9885 - val_loss: 0.0336\n",
      "Epoch 3/5\n",
      "1875/1875 - 20s - 11ms/step - accuracy: 0.9791 - loss: 0.0669 - val_accuracy: 0.9895 - val_loss: 0.0312\n",
      "Epoch 4/5\n",
      "1875/1875 - 25s - 13ms/step - accuracy: 0.9819 - loss: 0.0598 - val_accuracy: 0.9923 - val_loss: 0.0231\n",
      "Epoch 5/5\n",
      "1875/1875 - 23s - 12ms/step - accuracy: 0.9844 - loss: 0.0500 - val_accuracy: 0.9921 - val_loss: 0.0234\n",
      "Training with params: {'batch_size': 32, 'dense_units': 64, 'filters': (32, 64, 64), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "1875/1875 - 26s - 14ms/step - accuracy: 0.8845 - loss: 0.3692 - val_accuracy: 0.9785 - val_loss: 0.0737\n",
      "Epoch 2/5\n",
      "1875/1875 - 25s - 13ms/step - accuracy: 0.9615 - loss: 0.1246 - val_accuracy: 0.9874 - val_loss: 0.0398\n",
      "Epoch 3/5\n",
      "1875/1875 - 24s - 13ms/step - accuracy: 0.9728 - loss: 0.0872 - val_accuracy: 0.9908 - val_loss: 0.0282\n",
      "Epoch 4/5\n",
      "1875/1875 - 21s - 11ms/step - accuracy: 0.9782 - loss: 0.0706 - val_accuracy: 0.9913 - val_loss: 0.0265\n",
      "Epoch 5/5\n",
      "1875/1875 - 23s - 12ms/step - accuracy: 0.9813 - loss: 0.0610 - val_accuracy: 0.9918 - val_loss: 0.0276\n",
      "Training with params: {'batch_size': 32, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "1875/1875 - 37s - 20ms/step - accuracy: 0.9259 - loss: 0.2342 - val_accuracy: 0.9883 - val_loss: 0.0386\n",
      "Epoch 2/5\n",
      "1875/1875 - 41s - 22ms/step - accuracy: 0.9752 - loss: 0.0798 - val_accuracy: 0.9891 - val_loss: 0.0352\n",
      "Epoch 3/5\n",
      "1875/1875 - 40s - 21ms/step - accuracy: 0.9818 - loss: 0.0598 - val_accuracy: 0.9922 - val_loss: 0.0224\n",
      "Epoch 4/5\n",
      "1875/1875 - 37s - 20ms/step - accuracy: 0.9842 - loss: 0.0507 - val_accuracy: 0.9914 - val_loss: 0.0254\n",
      "Epoch 5/5\n",
      "1875/1875 - 39s - 21ms/step - accuracy: 0.9870 - loss: 0.0433 - val_accuracy: 0.9907 - val_loss: 0.0301\n",
      "Training with params: {'batch_size': 32, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "1875/1875 - 39s - 21ms/step - accuracy: 0.9081 - loss: 0.2895 - val_accuracy: 0.9860 - val_loss: 0.0424\n",
      "Epoch 2/5\n",
      "1875/1875 - 40s - 22ms/step - accuracy: 0.9717 - loss: 0.0934 - val_accuracy: 0.9874 - val_loss: 0.0376\n",
      "Epoch 3/5\n",
      "1875/1875 - 39s - 21ms/step - accuracy: 0.9781 - loss: 0.0702 - val_accuracy: 0.9912 - val_loss: 0.0287\n",
      "Epoch 4/5\n",
      "1875/1875 - 40s - 21ms/step - accuracy: 0.9831 - loss: 0.0545 - val_accuracy: 0.9917 - val_loss: 0.0251\n",
      "Epoch 5/5\n",
      "1875/1875 - 32s - 17ms/step - accuracy: 0.9849 - loss: 0.0481 - val_accuracy: 0.9923 - val_loss: 0.0242\n",
      "Training with params: {'batch_size': 32, 'dense_units': 128, 'filters': (32, 64, 64), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "1875/1875 - 24s - 13ms/step - accuracy: 0.9180 - loss: 0.2619 - val_accuracy: 0.9826 - val_loss: 0.0540\n",
      "Epoch 2/5\n",
      "1875/1875 - 22s - 12ms/step - accuracy: 0.9744 - loss: 0.0842 - val_accuracy: 0.9885 - val_loss: 0.0364\n",
      "Epoch 3/5\n",
      "1875/1875 - 24s - 13ms/step - accuracy: 0.9796 - loss: 0.0655 - val_accuracy: 0.9916 - val_loss: 0.0269\n",
      "Epoch 4/5\n",
      "1875/1875 - 26s - 14ms/step - accuracy: 0.9834 - loss: 0.0546 - val_accuracy: 0.9909 - val_loss: 0.0303\n",
      "Epoch 5/5\n",
      "1875/1875 - 25s - 14ms/step - accuracy: 0.9855 - loss: 0.0464 - val_accuracy: 0.9914 - val_loss: 0.0261\n",
      "Training with params: {'batch_size': 32, 'dense_units': 128, 'filters': (32, 64, 64), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "1875/1875 - 25s - 13ms/step - accuracy: 0.9008 - loss: 0.3151 - val_accuracy: 0.9836 - val_loss: 0.0510\n",
      "Epoch 2/5\n",
      "1875/1875 - 21s - 11ms/step - accuracy: 0.9664 - loss: 0.1085 - val_accuracy: 0.9903 - val_loss: 0.0291\n",
      "Epoch 3/5\n",
      "1875/1875 - 20s - 11ms/step - accuracy: 0.9757 - loss: 0.0790 - val_accuracy: 0.9855 - val_loss: 0.0455\n",
      "Epoch 4/5\n",
      "1875/1875 - 20s - 11ms/step - accuracy: 0.9804 - loss: 0.0647 - val_accuracy: 0.9921 - val_loss: 0.0258\n",
      "Epoch 5/5\n",
      "1875/1875 - 21s - 11ms/step - accuracy: 0.9825 - loss: 0.0568 - val_accuracy: 0.9915 - val_loss: 0.0242\n",
      "Training with params: {'batch_size': 32, 'dense_units': 128, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "1875/1875 - 37s - 20ms/step - accuracy: 0.9313 - loss: 0.2188 - val_accuracy: 0.9804 - val_loss: 0.0582\n",
      "Epoch 2/5\n",
      "1875/1875 - 33s - 17ms/step - accuracy: 0.9765 - loss: 0.0742 - val_accuracy: 0.9918 - val_loss: 0.0278\n",
      "Epoch 3/5\n",
      "1875/1875 - 33s - 17ms/step - accuracy: 0.9827 - loss: 0.0557 - val_accuracy: 0.9908 - val_loss: 0.0301\n",
      "Epoch 4/5\n",
      "1875/1875 - 33s - 18ms/step - accuracy: 0.9849 - loss: 0.0494 - val_accuracy: 0.9927 - val_loss: 0.0238\n",
      "Epoch 5/5\n",
      "1875/1875 - 34s - 18ms/step - accuracy: 0.9874 - loss: 0.0420 - val_accuracy: 0.9931 - val_loss: 0.0222\n",
      "Training with params: {'batch_size': 32, 'dense_units': 128, 'filters': (64, 128, 128), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "1875/1875 - 33s - 18ms/step - accuracy: 0.9182 - loss: 0.2630 - val_accuracy: 0.9888 - val_loss: 0.0365\n",
      "Epoch 2/5\n",
      "1875/1875 - 32s - 17ms/step - accuracy: 0.9735 - loss: 0.0863 - val_accuracy: 0.9893 - val_loss: 0.0352\n",
      "Epoch 3/5\n",
      "1875/1875 - 37s - 20ms/step - accuracy: 0.9808 - loss: 0.0609 - val_accuracy: 0.9856 - val_loss: 0.0463\n",
      "Epoch 4/5\n",
      "1875/1875 - 37s - 20ms/step - accuracy: 0.9847 - loss: 0.0502 - val_accuracy: 0.9934 - val_loss: 0.0226\n",
      "Epoch 5/5\n",
      "1875/1875 - 35s - 19ms/step - accuracy: 0.9864 - loss: 0.0443 - val_accuracy: 0.9922 - val_loss: 0.0233\n",
      "Training with params: {'batch_size': 64, 'dense_units': 64, 'filters': (32, 64, 64), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "938/938 - 21s - 23ms/step - accuracy: 0.8965 - loss: 0.3279 - val_accuracy: 0.9823 - val_loss: 0.0544\n",
      "Epoch 2/5\n",
      "938/938 - 20s - 22ms/step - accuracy: 0.9666 - loss: 0.1063 - val_accuracy: 0.9884 - val_loss: 0.0361\n",
      "Epoch 3/5\n",
      "938/938 - 22s - 23ms/step - accuracy: 0.9754 - loss: 0.0801 - val_accuracy: 0.9887 - val_loss: 0.0336\n",
      "Epoch 4/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.9798 - loss: 0.0651 - val_accuracy: 0.9872 - val_loss: 0.0418\n",
      "Epoch 5/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.9826 - loss: 0.0558 - val_accuracy: 0.9912 - val_loss: 0.0275\n",
      "Training with params: {'batch_size': 64, 'dense_units': 64, 'filters': (32, 64, 64), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.8618 - loss: 0.4318 - val_accuracy: 0.9686 - val_loss: 0.0994\n",
      "Epoch 2/5\n",
      "938/938 - 20s - 21ms/step - accuracy: 0.9566 - loss: 0.1415 - val_accuracy: 0.9870 - val_loss: 0.0408\n",
      "Epoch 3/5\n",
      "938/938 - 18s - 19ms/step - accuracy: 0.9686 - loss: 0.1010 - val_accuracy: 0.9885 - val_loss: 0.0365\n",
      "Epoch 4/5\n",
      "938/938 - 18s - 20ms/step - accuracy: 0.9743 - loss: 0.0844 - val_accuracy: 0.9910 - val_loss: 0.0277\n",
      "Epoch 5/5\n",
      "938/938 - 18s - 20ms/step - accuracy: 0.9785 - loss: 0.0702 - val_accuracy: 0.9906 - val_loss: 0.0300\n",
      "Training with params: {'batch_size': 64, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "938/938 - 30s - 32ms/step - accuracy: 0.9129 - loss: 0.2740 - val_accuracy: 0.9863 - val_loss: 0.0450\n",
      "Epoch 2/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9744 - loss: 0.0837 - val_accuracy: 0.9886 - val_loss: 0.0347\n",
      "Epoch 3/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9806 - loss: 0.0642 - val_accuracy: 0.9905 - val_loss: 0.0289\n",
      "Epoch 4/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9839 - loss: 0.0521 - val_accuracy: 0.9939 - val_loss: 0.0186\n",
      "Epoch 5/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9863 - loss: 0.0457 - val_accuracy: 0.9913 - val_loss: 0.0270\n",
      "Training with params: {'batch_size': 64, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "938/938 - 30s - 32ms/step - accuracy: 0.8864 - loss: 0.3641 - val_accuracy: 0.9807 - val_loss: 0.0579\n",
      "Epoch 2/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9670 - loss: 0.1074 - val_accuracy: 0.9853 - val_loss: 0.0469\n",
      "Epoch 3/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9770 - loss: 0.0757 - val_accuracy: 0.9897 - val_loss: 0.0310\n",
      "Epoch 4/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9798 - loss: 0.0631 - val_accuracy: 0.9913 - val_loss: 0.0255\n",
      "Epoch 5/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9830 - loss: 0.0533 - val_accuracy: 0.9940 - val_loss: 0.0225\n",
      "Training with params: {'batch_size': 64, 'dense_units': 128, 'filters': (32, 64, 64), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "938/938 - 20s - 21ms/step - accuracy: 0.9016 - loss: 0.3133 - val_accuracy: 0.9846 - val_loss: 0.0492\n",
      "Epoch 2/5\n",
      "938/938 - 18s - 19ms/step - accuracy: 0.9699 - loss: 0.0984 - val_accuracy: 0.9888 - val_loss: 0.0370\n",
      "Epoch 3/5\n",
      "938/938 - 18s - 19ms/step - accuracy: 0.9771 - loss: 0.0736 - val_accuracy: 0.9927 - val_loss: 0.0238\n",
      "Epoch 4/5\n",
      "938/938 - 18s - 19ms/step - accuracy: 0.9823 - loss: 0.0574 - val_accuracy: 0.9916 - val_loss: 0.0246\n",
      "Epoch 5/5\n",
      "938/938 - 18s - 19ms/step - accuracy: 0.9840 - loss: 0.0525 - val_accuracy: 0.9907 - val_loss: 0.0269\n",
      "Training with params: {'batch_size': 64, 'dense_units': 128, 'filters': (32, 64, 64), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "938/938 - 20s - 21ms/step - accuracy: 0.8690 - loss: 0.4151 - val_accuracy: 0.9795 - val_loss: 0.0637\n",
      "Epoch 2/5\n",
      "938/938 - 18s - 20ms/step - accuracy: 0.9593 - loss: 0.1343 - val_accuracy: 0.9863 - val_loss: 0.0442\n",
      "Epoch 3/5\n",
      "938/938 - 18s - 19ms/step - accuracy: 0.9699 - loss: 0.0972 - val_accuracy: 0.9877 - val_loss: 0.0349\n",
      "Epoch 4/5\n",
      "938/938 - 18s - 20ms/step - accuracy: 0.9755 - loss: 0.0790 - val_accuracy: 0.9895 - val_loss: 0.0314\n",
      "Epoch 5/5\n",
      "938/938 - 19s - 20ms/step - accuracy: 0.9796 - loss: 0.0669 - val_accuracy: 0.9921 - val_loss: 0.0245\n",
      "Training with params: {'batch_size': 64, 'dense_units': 128, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "938/938 - 30s - 32ms/step - accuracy: 0.9183 - loss: 0.2582 - val_accuracy: 0.9862 - val_loss: 0.0401\n",
      "Epoch 2/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9760 - loss: 0.0796 - val_accuracy: 0.9868 - val_loss: 0.0391\n",
      "Epoch 3/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9822 - loss: 0.0588 - val_accuracy: 0.9912 - val_loss: 0.0267\n",
      "Epoch 4/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9848 - loss: 0.0497 - val_accuracy: 0.9921 - val_loss: 0.0218\n",
      "Epoch 5/5\n",
      "938/938 - 28s - 29ms/step - accuracy: 0.9863 - loss: 0.0441 - val_accuracy: 0.9930 - val_loss: 0.0208\n",
      "Training with params: {'batch_size': 64, 'dense_units': 128, 'filters': (64, 128, 128), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "938/938 - 30s - 32ms/step - accuracy: 0.9001 - loss: 0.3171 - val_accuracy: 0.9833 - val_loss: 0.0473\n",
      "Epoch 2/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9695 - loss: 0.0985 - val_accuracy: 0.9885 - val_loss: 0.0335\n",
      "Epoch 3/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9772 - loss: 0.0733 - val_accuracy: 0.9886 - val_loss: 0.0352\n",
      "Epoch 4/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9810 - loss: 0.0609 - val_accuracy: 0.9905 - val_loss: 0.0300\n",
      "Epoch 5/5\n",
      "938/938 - 28s - 30ms/step - accuracy: 0.9840 - loss: 0.0517 - val_accuracy: 0.9898 - val_loss: 0.0264\n",
      "Training with params: {'batch_size': 128, 'dense_units': 64, 'filters': (32, 64, 64), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "469/469 - 19s - 41ms/step - accuracy: 0.8580 - loss: 0.4515 - val_accuracy: 0.9689 - val_loss: 0.0954\n",
      "Epoch 2/5\n",
      "469/469 - 18s - 37ms/step - accuracy: 0.9588 - loss: 0.1314 - val_accuracy: 0.9850 - val_loss: 0.0454\n",
      "Epoch 3/5\n",
      "469/469 - 18s - 37ms/step - accuracy: 0.9687 - loss: 0.1002 - val_accuracy: 0.9845 - val_loss: 0.0478\n",
      "Epoch 4/5\n",
      "469/469 - 18s - 37ms/step - accuracy: 0.9748 - loss: 0.0828 - val_accuracy: 0.9877 - val_loss: 0.0342\n",
      "Epoch 5/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9786 - loss: 0.0696 - val_accuracy: 0.9912 - val_loss: 0.0293\n",
      "Training with params: {'batch_size': 128, 'dense_units': 64, 'filters': (32, 64, 64), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "469/469 - 20s - 42ms/step - accuracy: 0.8160 - loss: 0.5933 - val_accuracy: 0.9691 - val_loss: 0.0986\n",
      "Epoch 2/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9400 - loss: 0.1901 - val_accuracy: 0.9782 - val_loss: 0.0693\n",
      "Epoch 3/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9579 - loss: 0.1361 - val_accuracy: 0.9848 - val_loss: 0.0450\n",
      "Epoch 4/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9657 - loss: 0.1095 - val_accuracy: 0.9876 - val_loss: 0.0408\n",
      "Epoch 5/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9712 - loss: 0.0954 - val_accuracy: 0.9882 - val_loss: 0.0332\n",
      "Training with params: {'batch_size': 128, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "469/469 - 28s - 60ms/step - accuracy: 0.8860 - loss: 0.3636 - val_accuracy: 0.9833 - val_loss: 0.0533\n",
      "Epoch 2/5\n",
      "469/469 - 26s - 56ms/step - accuracy: 0.9670 - loss: 0.1049 - val_accuracy: 0.9889 - val_loss: 0.0336\n",
      "Epoch 3/5\n",
      "469/469 - 26s - 56ms/step - accuracy: 0.9772 - loss: 0.0730 - val_accuracy: 0.9857 - val_loss: 0.0452\n",
      "Epoch 4/5\n",
      "469/469 - 26s - 56ms/step - accuracy: 0.9814 - loss: 0.0611 - val_accuracy: 0.9917 - val_loss: 0.0237\n",
      "Epoch 5/5\n",
      "469/469 - 26s - 56ms/step - accuracy: 0.9841 - loss: 0.0511 - val_accuracy: 0.9909 - val_loss: 0.0268\n",
      "Training with params: {'batch_size': 128, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "469/469 - 28s - 59ms/step - accuracy: 0.8522 - loss: 0.4673 - val_accuracy: 0.9773 - val_loss: 0.0823\n",
      "Epoch 2/5\n",
      "469/469 - 26s - 56ms/step - accuracy: 0.9581 - loss: 0.1391 - val_accuracy: 0.9810 - val_loss: 0.0580\n",
      "Epoch 3/5\n",
      "469/469 - 28s - 59ms/step - accuracy: 0.9694 - loss: 0.0968 - val_accuracy: 0.9884 - val_loss: 0.0359\n",
      "Epoch 4/5\n",
      "469/469 - 27s - 58ms/step - accuracy: 0.9764 - loss: 0.0777 - val_accuracy: 0.9908 - val_loss: 0.0296\n",
      "Epoch 5/5\n",
      "469/469 - 27s - 57ms/step - accuracy: 0.9776 - loss: 0.0703 - val_accuracy: 0.9874 - val_loss: 0.0386\n",
      "Training with params: {'batch_size': 128, 'dense_units': 128, 'filters': (32, 64, 64), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "469/469 - 19s - 41ms/step - accuracy: 0.8659 - loss: 0.4213 - val_accuracy: 0.9791 - val_loss: 0.0684\n",
      "Epoch 2/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9611 - loss: 0.1253 - val_accuracy: 0.9867 - val_loss: 0.0398\n",
      "Epoch 3/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9720 - loss: 0.0884 - val_accuracy: 0.9838 - val_loss: 0.0501\n",
      "Epoch 4/5\n",
      "469/469 - 18s - 37ms/step - accuracy: 0.9768 - loss: 0.0760 - val_accuracy: 0.9912 - val_loss: 0.0281\n",
      "Epoch 5/5\n",
      "469/469 - 17s - 37ms/step - accuracy: 0.9811 - loss: 0.0608 - val_accuracy: 0.9880 - val_loss: 0.0377\n",
      "Training with params: {'batch_size': 128, 'dense_units': 128, 'filters': (32, 64, 64), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "469/469 - 19s - 41ms/step - accuracy: 0.8218 - loss: 0.5610 - val_accuracy: 0.9704 - val_loss: 0.0957\n",
      "Epoch 2/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9483 - loss: 0.1709 - val_accuracy: 0.9766 - val_loss: 0.0684\n",
      "Epoch 3/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9640 - loss: 0.1198 - val_accuracy: 0.9874 - val_loss: 0.0406\n",
      "Epoch 4/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9708 - loss: 0.0942 - val_accuracy: 0.9873 - val_loss: 0.0394\n",
      "Epoch 5/5\n",
      "469/469 - 18s - 38ms/step - accuracy: 0.9750 - loss: 0.0824 - val_accuracy: 0.9889 - val_loss: 0.0359\n",
      "Training with params: {'batch_size': 128, 'dense_units': 128, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n",
      "Epoch 1/5\n",
      "469/469 - 28s - 60ms/step - accuracy: 0.8967 - loss: 0.3246 - val_accuracy: 0.9859 - val_loss: 0.0440\n",
      "Epoch 2/5\n",
      "469/469 - 27s - 57ms/step - accuracy: 0.9711 - loss: 0.0928 - val_accuracy: 0.9889 - val_loss: 0.0352\n",
      "Epoch 3/5\n",
      "469/469 - 26s - 56ms/step - accuracy: 0.9789 - loss: 0.0672 - val_accuracy: 0.9884 - val_loss: 0.0377\n",
      "Epoch 4/5\n",
      "469/469 - 27s - 57ms/step - accuracy: 0.9827 - loss: 0.0549 - val_accuracy: 0.9920 - val_loss: 0.0260\n",
      "Epoch 5/5\n",
      "469/469 - 28s - 61ms/step - accuracy: 0.9852 - loss: 0.0480 - val_accuracy: 0.9938 - val_loss: 0.0190\n",
      "Training with params: {'batch_size': 128, 'dense_units': 128, 'filters': (64, 128, 128), 'learning_rate': 0.0005}\n",
      "Epoch 1/5\n",
      "469/469 - 30s - 64ms/step - accuracy: 0.8648 - loss: 0.4329 - val_accuracy: 0.9768 - val_loss: 0.0731\n",
      "Epoch 2/5\n",
      "469/469 - 29s - 61ms/step - accuracy: 0.9618 - loss: 0.1246 - val_accuracy: 0.9854 - val_loss: 0.0457\n",
      "Epoch 3/5\n",
      "469/469 - 28s - 60ms/step - accuracy: 0.9726 - loss: 0.0870 - val_accuracy: 0.9896 - val_loss: 0.0332\n",
      "Epoch 4/5\n",
      "469/469 - 27s - 58ms/step - accuracy: 0.9783 - loss: 0.0708 - val_accuracy: 0.9915 - val_loss: 0.0273\n",
      "Epoch 5/5\n",
      "469/469 - 28s - 60ms/step - accuracy: 0.9815 - loss: 0.0603 - val_accuracy: 0.9894 - val_loss: 0.0331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.9939000010490417 with params: {'batch_size': 64, 'dense_units': 64, 'filters': (64, 128, 128), 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Run grid search\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for params in grid:\n",
    "    acc, model = train_and_evaluate(params)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best Accuracy: {best_acc} with params: {best_params}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_mnist_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Classifying large images using Inception v3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Download some images of various animals. Load them in Python, for example\n",
    "using the matplotlib.image.mpimg.imread() function. Resize and/or crop\n",
    "them to 299 Ã— 299 pixels, and ensure that they have just three channels (RGB),\n",
    "with no transparency channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [299, 299])  # Redimensionar para 299x299\n",
    "    image = tf.image.grayscale_to_rgb(image) if image.shape[-1] == 1 else image  # Garantir 3 canais\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalizar para [0, 1]\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Administrator\\tensorflow_datasets\\oxford_iiit_pet\\4.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "dataset_name = \"oxford_iiit_pet\"\n",
    "dataset, info = tfds.load(dataset_name, split=\"train[:20]\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some images\n",
    "batch_size = 5\n",
    "batched_dataset = processed_dataset.batch(batch_size)\n",
    "images, labels = next(iter(batched_dataset))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(batch_size):\n",
    "    ax = plt.subplot(1, batch_size, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Label: {labels[i].numpy()}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use the Pretrained Inception v3 Model from TensorFlow/Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InceptionV3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mInceptionV3\u001b[49m(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrained model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InceptionV3' is not defined"
     ]
    }
   ],
   "source": [
    "model = InceptionV3(weights=\"imagenet\")\n",
    "model.summary()\n",
    "print(\"Pretrained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Create the Inception v3 model by calling the inception_v3() function, as\n",
    "shown below. This must be done within an argument scope created by the\n",
    "inception_v3_arg_scope() function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
